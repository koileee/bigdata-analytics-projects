{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Overview\n",
    "\n",
    "\n",
    "If two events $x$ and $y$ are independent, their PMI will be zero.   A positive PMI indicates that $x$ and $y$ are more likely to co-occur than they would be if they were independent.   Similarly, a negative PMI indicates that $x$ and $y$ are less likely to co-occur.   The PMI of events $x$ and $y$ is given by\n",
    "\\begin{equation*}\n",
    "PMI(x,y) = \\log\\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{equation*}\n",
    "where $p(x)$ and $p(y)$ are the probabilities of occurrence of events $x$ and $y$, and $p(x,y)$ is the probability of co-occurrence of $x$ and $y$.\n",
    "\n",
    "We are interested in are occurrences of tokens on lines of text in the input file.   For example, one event\n",
    "might represent the occurence of the token \"fire\" a line of text, and another might represent the occurrence of the token \"peace\".   In that case, $p(fire)$ represents the probability that \"fire\" will occur on a line of text, and $p(fire,peace)$ represents the probability that *both* \"fire\" and \"peace\" will occur on the *same* line.   For the purposes of these PMI computations, it does not matter how many times a given token occures on a single line.   Either a line contains a particular token (at least once), or it does not.   For example, consider this line of text:\n",
    "\n",
    "> three three three, said thrice\n",
    "\n",
    "For this line, the following token-pair events have occurred:\n",
    "- (three, said)\n",
    "- (three, thrice)\n",
    "- (said, three)\n",
    "- (said, thrice)\n",
    "- (thrice, three)\n",
    "- (thrice, said)\n",
    "\n",
    "Note that we are not interested in \"reflexive\" pairs, such as (thrice,thrice).\n",
    "\n",
    "In addition to the probabilities of events, we will also be interested in the absolute *number* of occurences of particular events, e.g., the number of lines in which \"fire\" occurs.   We will use $n(x)$ to represent the these numbers.\n",
    "\n",
    "Your main task for this assignment is to write Python code to analyze the PMI of tokens from Shakespeare's plays.    Based this analysis, we want to be able to answer two types of queries:\n",
    "\n",
    "* Two-Token Queries: Given a pair of tokens, $x$ and $y$, report the number of lines on which that pair co-occurs ($n(x,y)$) as well as $PMI(x,y)$.\n",
    "* One-Token Queries: Given a single token, $x$, report the number of lines on which that token occurs ($n(x)$).   In addition, report the five tokens that have the largest PMI with respect to $x$ (and their PMIs).   That is, report the five $y$'s for which $PMI(x,y)$ is largest.\n",
    "\n",
    "To avoid reporting spurious results for the one-token queries, we are only interested in token pairs that co-occur a sufficient number of times.   Therefore, we will use a *threshold* parameter for one-token queries.   A one-token query should only report pairs of tokens that co-occur at least *threshold* times in the input.   For example, given the threshold 12, a one-token query for \"fire\" the should report the five tokens that have the largest PMI (with respect to \"fire\") among all tokens that co-occur with \"fire\" on at least 12 lines.   If there are fewer than five such tokens, report fewer than five.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tokens:  25975\n",
      "distinct token pairs:  1969760\n"
     ]
    }
   ],
   "source": [
    "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
    "from simple_tokenize import simple_tokenize\n",
    "\n",
    "distinct_tokens={}\n",
    "distinct_pairs={}\n",
    "\n",
    "# Now, let's tokenize Shakespeare's plays\n",
    "with open('Shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # tokenize, one line at a time\n",
    "        t = simple_tokenize(line)\n",
    "        for token in t:\n",
    "            distinct_tokens[token] = distinct_tokens.get(token, 0)\n",
    "            for t2 in t:\n",
    "                if token != t2:\n",
    "                    distinct_pairs[(token,t2)] = distinct_pairs.get((token,t2), 0)\n",
    "\n",
    "print(\"distinct tokens: \", len(distinct_tokens))\n",
    "print(\"distinct token pairs: \", len(distinct_pairs))\n",
    "# extend this code to answer Question 1.\n",
    "# when your code is executed, it should print the number of distinct tokens and the number of distinct token pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1 or 2 space-separated tokens (return to quit): love\n",
      "Input a positive integer frequency threshold: 40\n",
      "  n(love) = 2020\n",
      "  high PMI tokens with respect to love (threshold: 40):\n",
      " n(love,true) = 48,  PMI(love,true) = 0.561\n",
      " n(love,thee) = 126,  PMI(love,thee) = 0.399\n",
      " n(love,her) = 137,  PMI(love,her) = 0.381\n",
      " n(love,do) = 130,  PMI(love,do) = 0.337\n",
      " n(love,if) = 122,  PMI(love,if) = 0.331\n",
      "Input 1 or 2 space-separated tokens (return to quit): love hate\n",
      "  n(love,hate) = 34.000\n",
      "  PMI(love,hate) = 1.074\n",
      "Input 1 or 2 space-separated tokens (return to quit): lalalla\n",
      "Input a positive integer frequency threshold: 60\n",
      "  n(lalalla) = 0\n",
      "  high PMI tokens with respect to lalalla (threshold: 60):\n",
      " None\n",
      "Input 1 or 2 space-separated tokens (return to quit): cool you\n",
      "  n(cool,you) = 1.000\n",
      "  PMI(cool,you) = -0.490\n",
      "Input 1 or 2 space-separated tokens (return to quit): lulu lala\n",
      " one of the tokens do not exist\n",
      "Input 1 or 2 space-separated tokens (return to quit): you lala\n",
      " one of the tokens do not exist\n",
      "Input 1 or 2 space-separated tokens (return to quit): \n"
     ]
    }
   ],
   "source": [
    "# this imports the SimpleTokenize function from the simple_tokenize.py file that you uploaded\n",
    "from simple_tokenize import simple_tokenize\n",
    "# the log function for computing PMI\n",
    "# for the sake of consistency across solutions, please use log base 10\n",
    "from math import log\n",
    "\n",
    "###################################################################################################################\n",
    "#  replace this with your PMI analysis code, so that you can support the user interface below\n",
    "#  it should read and tokenize Shakespeare.txt, and store enough information in Python data structures\n",
    "#  to allow you to answer the PMI queries below\n",
    "###################################################################################################################\n",
    "\n",
    "###################################################################################################################\n",
    "#  the user interface below defines the types of PMI queries that users can ask\n",
    "#  you will need to modify it - where indicated - to access the results of your PMI analysis (above)\n",
    "#  so that the queries can be answered\n",
    "###################################################################################################################\n",
    "\n",
    "tokens={}\n",
    "token_pairs={}\n",
    "totalLines = 0\n",
    "# Now, let's tokenize Shakespeare's plays\n",
    "\n",
    "\n",
    "with open('Shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # tokenize, one line at a time\n",
    "        t = simple_tokenize(line)\n",
    "        totalLines+=1\n",
    "        # find distinct tokens\n",
    "        set_t = set(t)\n",
    "        # for each distinct token in the set, add 1 to its occurence\n",
    "        for token in set_t:\n",
    "            tokens[token] = tokens.get(token, 0) + 1\n",
    "            # for each distinct token pair in the set, add 1 to its occurence\n",
    "            for t2 in set_t:\n",
    "                if token != t2:\n",
    "                    token_pairs[token] = token_pairs.get(token, {})\n",
    "                    token_pairs[token][(token,t2)] = token_pairs[token].get((token,t2),0)+1\n",
    "\n",
    "# helper PMI function\n",
    "def pmi(v,pq):\n",
    "    p = tokens[v[0]]/totalLines\n",
    "    q = tokens[v[1]]/totalLines\n",
    "    return log((pq/totalLines)/(p*q), 10)\n",
    "\n",
    "\n",
    "while True:\n",
    "    q = input(\"Input 1 or 2 space-separated tokens (return to quit): \")\n",
    "    if len(q) == 0:\n",
    "        break\n",
    "    q_tokens = simple_tokenize(q)\n",
    "    if len(q_tokens) == 1:\n",
    "        threshold = 0\n",
    "        while threshold <= 0:\n",
    "            try:\n",
    "                threshold = int(input(\"Input a positive integer frequency threshold: \"))\n",
    "            except ValueError:\n",
    "                print(\"Threshold must be a positive integer!\")\n",
    "                continue\n",
    "        number = 0\n",
    "        top5 ={}\n",
    "        if q_tokens[0] in tokens:\n",
    "            number = tokens[q_tokens[0]]\n",
    "            # for each token pair associated with token, filter those that are above the threshold\n",
    "            filtered_dict = {k:v for (k,v) in token_pairs[q_tokens[0]].items() if v >= threshold}\n",
    "            # create another dictionary for PMI\n",
    "            pmi_dict = {k:pmi(k,v) for (k,v) in filtered_dict.items()}\n",
    "            # top 5 largest PMI\n",
    "            top5 = dict(sorted(pmi_dict.items(),key=lambda item: item[1], reverse=True)[:5])\n",
    "        print(\"  n({0}) = {1}\".format(q_tokens[0], number ))\n",
    "        print(\"  high PMI tokens with respect to {0} (threshold: {1}):\".format(q_tokens[0],threshold))\n",
    "        if len(top5) == 0:\n",
    "            print(\" None\")\n",
    "        for item in top5:\n",
    "            print(\" n({0},{1}) = {2},  PMI({0},{1}) = {3:.3f}\".format(q_tokens[0], item[1], filtered_dict[item], top5[item])) \n",
    "    elif len(q_tokens) == 2:\n",
    "        # PMI calculation\n",
    "        if (q_tokens[0] in tokens) and (q_tokens[1] in tokens):\n",
    "            p = tokens[q_tokens[0]]/totalLines\n",
    "            q = tokens[q_tokens[1]]/totalLines\n",
    "            if (q_tokens[0],q_tokens[1]) in token_pairs[q_tokens[0]]:\n",
    "                pq = token_pairs[q_tokens[0]][(q_tokens[0],q_tokens[1])]\n",
    "                PMI = log((pq/totalLines)/(p*q), 10)  \n",
    "                # prints\n",
    "                print(\"  n({0},{1}) = {2:.3f}\".format(q_tokens[0],q_tokens[1], pq))\n",
    "                print(\"  PMI({0},{1}) = {2:.3f}\".format(q_tokens[0],q_tokens[1], PMI))\n",
    "            else:\n",
    "                print (\" token pair does not exist\")\n",
    "        else:\n",
    "            print(\" one of the tokens do not exist\")\n",
    "    else:\n",
    "        print(\"Input must consist of 1 or 2 space-separated tokens!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}