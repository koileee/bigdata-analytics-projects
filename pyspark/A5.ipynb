{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1gm9-t0i7Ok"
      },
      "source": [
        "## CS431/631 Data Intensive Distributed Computing\n",
        "### Fall 2020 - Assignment 5\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CB3G_gbi7Ok"
      },
      "source": [
        "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
        "* **Name:** Yu Li\n",
        "* **ID:** 20603929"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5BLErorljlr"
      },
      "source": [
        "Unlike our previous environment, Spark is not installed in Colab so we have to install it ourself. This will take a minute to finish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh5NCoc8fsSO"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILheUROOhprv"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Next we need to create SparkContext. Run the following block to set the environment path and create a SparkContext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnxe-BhPmbBW"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(appName=\"YourTest\", master=\"local[*]\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJ_8b5Li7Ol"
      },
      "source": [
        "---\n",
        "#### Overview\n",
        "For this assignment, you will be using Python and Spark to perform spam detection.   You will need to perform two tasks.   The first is to build spam prediction models, using training data sets and stochastic gradient descent (SGD).   The second is to use these models to predict whether the documents in a test data set are spam.\n",
        "The stochastic gradient descent technique that you will be using is based on [a paper](http://arxiv.org/abs/1004.5168) by Cormack, Smucker and Clarke.\n",
        "\n",
        "#### Training a Spam Classification Models\n",
        "To build a spam classification model, you will start with a training data set.   Each instance in the training set represents a single document, and is labeled to indicate whether that document should be considered to be spam or ham.\n",
        "An instance looks like this:\n",
        "```\n",
        "clueweb09-en0094-20-13546 spam 387908 697162 426572 161118 688171 ...\n",
        "```\n",
        "The first field, `clueweb09-en0094-20-13546`, is the (unique) document name.   The second field is the label, indicating whether the document should be considered spam (as in this example) or ham.   The remaining fields are integers representing *features* present in the document.   In this case, the features are hashed byte 4-grams, represented as integers.   Each training data set is stored as a text file, with one instance per line.   The training files  are:\n",
        "* `spam.train.group_x.txt`   (25 MB)\n",
        "* `spam.train.group_y.txt`   (20 MB)\n",
        "* `spam.train.britney.txt`   (766 MB)\n",
        "\n",
        "Now let's download the spamminess module and the training traces we will use in this assignment. This will take a few minutes. The ls command at the end shows the files we have in this directory. Make sure all files are here now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GrsO9SjnwEP",
        "outputId": "8d682356-8bf7-4b11-c801-610d1c7c86c1"
      },
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/W20/content/cs431/spamminess.py\n",
        "!wget -q https://www.student.cs.uwaterloo.ca/~cs451/spam/spam.train.group_x.txt.bz2\n",
        "!wget -q https://www.student.cs.uwaterloo.ca/~cs451/spam/spam.train.group_y.txt.bz2\n",
        "!wget -q https://www.student.cs.uwaterloo.ca/~cs451/spam/spam.train.britney.txt.bz2\n",
        "\n",
        "!bunzip2 spam.train.group_x.txt.bz2\n",
        "!bunzip2 spam.train.group_y.txt.bz2\n",
        "!bunzip2 spam.train.britney.txt.bz2\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\tspam.train.group_y.txt\n",
            "spamminess.py\t\tspark-2.4.7-bin-hadoop2.7\n",
            "spam.train.britney.txt\tspark-2.4.7-bin-hadoop2.7.tgz\n",
            "spam.train.group_x.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyIvJDwBnkSa"
      },
      "source": [
        "\n",
        "---\n",
        "### Important\n",
        "\n",
        "The questions that follow ask you to implement functions whose prototypes are given to you. Do _**not**_ change the prototypes of the functions. Do _**not**_ write code outside of the functions.\n",
        "\n",
        "You may use specific cells, identified by `# Your tests here`, for test purposes. Code in these cells will *not* be executed when marking your assignment.\n",
        "\n",
        "---\n",
        "\n",
        "#### Question 1 ( 5/20 marks)\n",
        "\n",
        "Your first task is to write a sequential SGD model trainer in Python (no Spark).   For our purposes, a model associates a *weight* with each feature.   The model trainer decides what these weights should be, based on the training instances.  Since you are going to be writing a model trainer based on SGD, the trainer should behave like this:\n",
        "```\n",
        "for each training instance T\n",
        "   predict whether T is spam or ham using the weights of the current model\n",
        "   update the model weights by comparing T's predicted label with its actual label\n",
        "```\n",
        "Of course, the important part is how to update the model.\n",
        "\n",
        "In [the paper](http://arxiv.org/abs/1004.5168), the model is used to assign a \"spamminess\" score to a document.   Documents with positive spamminess are predicted to be spam.   Those with negative spamminess are predicted to be ham.  The spamminess of a document $D$ is simply the sum of the weights (from the model) of each of the document's features:\n",
        "\\begin{equation}\n",
        "spamminess(D) = \\sum_{f \\in D}{w(f)}\n",
        "\\end{equation}\n",
        "where $w(f)$ is the weight assocated with feature $f$.\n",
        "\n",
        "The Python module `spamminess.py` defines a function `spamminess(F,W)` which computes this quantity.   This function takes two arguments, `F` and `W`.  `F` is a list of features (integers) associated to the document whose spamminess you want to compute, and `W` is a dictionary representing the current model.  `W` maps features ($f$) to their weights ($w(f)$) under the model.\n",
        "\n",
        "In the cell below, you will find partial pseudo-code that shows how to implement the SGD model trainer defined by Cormack, Smucker, and Clarke.   It reads the training instances one at a time from one of the training files, and uses them to adjust the model weights.   Your job is to turn this pseudo-code into actual runnable Python code that can\n",
        "be used to learn a model from any one of the training files. Implement the function `sequential_SGD()` that takes as input a model (`w`), the training dataset and a value for the update parameter `delta`, and returns the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVoQ1DSci7On"
      },
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "\n",
        "def sequential_SGD(model, training_dataset='spam.train.group_x.txt', delta = 0.002):\n",
        "    # open one of the training files - defaults to group_x\n",
        "    i = 0\n",
        "    with open(training_dataset) as f:\n",
        "      for line in f:\n",
        "        lines = line.split()\n",
        "        t = lines[1]\n",
        "        F = lines[2:]\n",
        "        score = spamminess(F, model)\n",
        "        prob = 1/(1+exp(-score))\n",
        "        for ft in F:\n",
        "          if t == \"spam\":\n",
        "            model[ft] = model.get(ft,0) + (1.0-prob)*delta\n",
        "          elif t == \"ham\":\n",
        "            model[ft] = model.get(ft,0) - prob*delta\n",
        "    return model\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAzF-80Yi7On"
      },
      "source": [
        "# Your tests here\n",
        "w = sequential_SGD({}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiyidVMdi7On"
      },
      "source": [
        "#### Question 2 (5/20 marks)\n",
        "\n",
        "Next, you should try implementing a Spark version of the SGD model trainer.   Your Spark implementation should read a training file, train the model, and then output the model to the `models` folder.  The model output file that you generate should list the weight associated with each feature, with one feature per line, like this:\n",
        "```\n",
        "(802123, 0.0009858585991850937)\n",
        "(438450, 4.267897922108138e-05)\n",
        "(271525, 0.0013133437007968654)\n",
        "(92853, 0.0004300009932503611)\n",
        "```\n",
        "\n",
        "Use Spark's `saveAsTextFile` action to output your model.   For example, if you are training a model for the group_x training set, use `saveAsTextFile(\"models/group_x_model\")`.   This will actually cause Spark to create a folder called `group_x_model`.   In the folder, there will be files with names like `part-00000` that contain the actual output data.  When you use `saveAsTextFile`, Spark will generate one `part-xxxxx` file for each partition of the RDD that you are writing out.   In this case, you should have only a single partition (for the reason described below), so there should be only one `part-xxxxx` file.\n",
        "\n",
        "Training the SGD model is an inherently sequential task, since the training instances update the model one at a time, and each instance's spamminess is computed using the model produced by that instance's predecessors.   This means that the only part of the training that you can parallelize using Spark is the parsing of the input file.   Once the input is parsed, your Spark implementation will have to force all of the instances into a single partition, and then apply the training function to the entire partition.   To see whether you are getting sensible results, you can compare the model you learn with Spark to the one that you learned with your sequential Python program from Question 1.\n",
        "\n",
        "Remember that training should occur entirely in Spark.  The training instances should never come into your driver program.\n",
        "\n",
        "Implement the function `spark_SGD()` below that takes as input the path to the training dataset, an output path `output_model` and a value for the update parameter `delta`, and writes the trained model to `output_model` using Spark's `saveAsTextFile`. You can use it to generate models from all three of the training files, leaving the results in your models folder. For this assignment, you will be using Spark's original RDD interface, rather than the DataFrame interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd2wZ_Rji7On"
      },
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "import shutil, os\n",
        "\n",
        "def spark_SGD(training_dataset='spam.train.group_x.txt', output_model='models/group_x_model', delta = 0.002):\n",
        "    if os.path.isdir(output_model):\n",
        "        shutil.rmtree(output_model) # Remove the previous model to create a new one\n",
        "    training_data = sc.textFile(training_dataset)\n",
        "    # transform function\n",
        "    def transform(line):\n",
        "      l=line.split(\" \")\n",
        "      return (l[1], l[2:])\n",
        "    # train function, yields trained model\n",
        "    def train(trainingData):\n",
        "        model = {}\n",
        "        for t in trainingData:\n",
        "            features = t[1]\n",
        "            score = spamminess(features,model)\n",
        "            prob = 1.0/(1+exp(-score))\n",
        "            for f in features:\n",
        "                if t[0] == 'spam':\n",
        "                  model[f]=model.get(f,0)+(1.0-prob)*delta\n",
        "                else:\n",
        "                  model[f] =model.get(f,0) -prob*delta\n",
        "        yield model\n",
        "    # transform the data and have one partition\n",
        "    grouped = training_data.map(lambda x: transform(x)).coalesce(1)\n",
        "    # train on the single partition\n",
        "    final = grouped.mapPartitions(train).flatMap(lambda x: x.items())\n",
        "    # save final result\n",
        "    final.saveAsTextFile(output_model)\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhKu2ewai7Oo"
      },
      "source": [
        "# Your tests here\n",
        "spark_SGD()\n",
        "spark_SGD(training_dataset='spam.train.group_y.txt', output_model='models/group_y_model')\n",
        "spark_SGD(training_dataset='spam.train.britney.txt', output_model='models/britney_model')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmwTvAsvi7Oo"
      },
      "source": [
        "#### Question 3 (5/20 marks)\n",
        "\n",
        "When you train a model using SGD, the model you get depends on the order in which you handle the training instances.  To see this in action, try using the Spark SGD trainer you implemented for Question 2 to train a model from the group_x training set, but with the instances processed in a different order.  \n",
        "\n",
        "To do this, re-implement your trainer from Question 2 so that it will randomly reorder the training instances before using them to update the model. One way to shuffle the training instances is to assign a random sort key to each training instance as you read it from the input file, and then sort the instances using the random sort key.\n",
        "\n",
        "Be sure that Spark is doing the work of shuffling the training instances.   Do not load the training instances into your driver program and sort them there.\n",
        "\n",
        "Implement the function `spark_shuffled_SGD` below that takes as input the path to the training dataset, an output path `output_model` and a value for the update parameter `delta`, shuffles the training instances using the method described above and writes the trained model to `output_model` using Spark's `saveAsTextFile`.\n",
        "\n",
        "Once you have implemented the shuffled trainer, train a model using shuffled group_x training instances, and compare the resulting model with group_x model you learned without shuffling.  It is up to you how to do this comparision.  At a minimum, compare features with the highest weights in each model to see if they are similar. You can also use the classifier in next question to classify documents using the two models, and compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXU4FPMvi7Oo"
      },
      "source": [
        "from spamminess import spamminess\n",
        "from math import exp\n",
        "import shutil, os, random\n",
        "\n",
        "def spark_shuffled_SGD(training_dataset='spam.train.group_x.txt', output_model='models/group_x_model', delta = 0.002):\n",
        "    if os.path.isdir(output_model):\n",
        "        shutil.rmtree(output_model) # Remove the previous model to create a new one\n",
        "    training_data = sc.textFile(training_dataset)\n",
        "    #### transform helper\n",
        "    def transform(line):\n",
        "      l=line[1].split(\" \")\n",
        "      return (l[1], l[2:])\n",
        "    # training function\n",
        "    def train(trainingData):\n",
        "        model = {}\n",
        "        for t in trainingData:\n",
        "            features = t[1]\n",
        "            score = spamminess(features,model)\n",
        "            prob = 1.0/(1+exp(-score))\n",
        "            for f in features:\n",
        "                if t[0] == 'spam':\n",
        "                  model[f]=model.get(f,0)+(1.0-prob)*delta\n",
        "                else:\n",
        "                  model[f] =model.get(f,0) -prob*delta\n",
        "        yield model\n",
        "    # assign random number to each document and sort\n",
        "    training_data = training_data.map(lambda line: (random.random(), line)).sortByKey()\n",
        "    # perform transformation on shuffled data\n",
        "    grouped = training_data.map(lambda x: transform(x)).coalesce(1)\n",
        "    # return final model\n",
        "    final = grouped.mapPartitions(train).flatMap(lambda x: x.items())\n",
        "    # save model\n",
        "    final.saveAsTextFile(output_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8UeA0Wdi7Oo"
      },
      "source": [
        "# Your tests here\n",
        "spark_shuffled_SGD(output_model='models/group_x_model_shuffled')\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpvLuHe4puP7"
      },
      "source": [
        "#### Question 4 (5/20  marks)\n",
        "\n",
        "Last but not least, you should write a Spark program that can be used to classify documents as spam or ham, using the classification models you produced.\n",
        "\n",
        "The test data, i.e., the document instances that you should classifiy, are located in `spam.test.qrels.txt`. Run the following block to download this trace. This will take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L41oN0Wipvl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d750983d-1530-405e-b0e5-d274d72dc8b1"
      },
      "source": [
        "!wget -q https://www.student.cs.uwaterloo.ca/~cs451/spam/spam.test.qrels.txt.bz2\n",
        "!bunzip2 spam.test.qrels.txt.bz2\n",
        "!ls"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "models\t       spam.test.qrels.txt     spark-2.4.7-bin-hadoop2.7\n",
            "__pycache__    spam.train.britney.txt  spark-2.4.7-bin-hadoop2.7.tgz\n",
            "sample_data    spam.train.group_x.txt\n",
            "spamminess.py  spam.train.group_y.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTYn-6ssi7Oo"
      },
      "source": [
        "\n",
        "Each line in this file represents a document that needs to be classified as spam or ham.  The format of this file is identical to the format of the files that hold the training instances.\n",
        "\n",
        "Implement the function `spark_classify` below that will load a model (from a specified folder under `models`), classify all of the instances in a given test data file (`spam.test.qrels.txt` by default) using that model, and then output the results in the folder `results_path` using Spark's `saveAsTextFile`.   The contents of the output file should look like this:\n",
        "```\n",
        "(clueweb09-en0000-00-00142,spam,2.601624279252943,spam)\n",
        "(clueweb09-en0000-00-01005,ham,2.5654162439491004,spam)\n",
        "(clueweb09-en0000-00-01382,ham,2.5893946346394188,spam)\n",
        "```\n",
        "Each line of the output represents one test instance.   The first two fields are the document ID and the test label.  These are just copied from the test data.   The third field is the spamminess score of the document, produced by the spamminess function using the model you are classifying with.   The fourth field is the spam/ham prediction made by the model.\n",
        "\n",
        "Of course, your spam/ham classifier must **not** use the test label from the input when making its prediction.  The test labels are the \"ground truth\" against which your predictions are being compared.   Using them to make predictions would defeat the whole purpose of model-based classification.\n",
        "\n",
        "Make sure that classification of the test instances is done by Spark, not by your driver program.  Do ***not*** load the test instances or classification results into your driver program. You are however allowed to load the model weights into your driver program to distribute them as side data. \n",
        "Unlike model training, classification is easily parallelizable, since each document is classified independently. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-DK5bPgi7Oo"
      },
      "source": [
        "from spamminess import spamminess\n",
        "import shutil, os\n",
        "def spark_classify(input_model='models/group_x_model', test_dataset='spam.test.qrels.txt', results_path='results/test_qrels'):\n",
        "\n",
        "    if os.path.isdir(results_path):\n",
        "        shutil.rmtree(results_path) # Remove the previous results\n",
        "    test_data = sc.textFile(test_dataset)\n",
        "    model = sc.textFile(input_model+'/part-00000')\n",
        "    # format the model to transform into a dicitonary\n",
        "    def modelFormat(line):\n",
        "      l=line[1:-1].split(\",\")\n",
        "      return (l[0][1:-1],float(l[1]))\n",
        "    model = model.map(modelFormat).collectAsMap()\n",
        "    # testing process function\n",
        "    def test(line):\n",
        "      l = line.split(\" \")\n",
        "      score = spamminess(l[2:],model)\n",
        "      predict=\"ham\"\n",
        "      if score > 0:\n",
        "        predict=\"spam\"\n",
        "      return(l[0], l[1], spamminess(l[2:],model), predict)\n",
        "    # test result saved\n",
        "    test_data.map(test).saveAsTextFile(results_path)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-FkKAhcqgHl"
      },
      "source": [
        "We have developed a program that can be used to evaluate your classification results.  Run the next block to download this program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uRP5o9ZqhAd"
      },
      "source": [
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/compute_spam_metrics.c\n",
        "!wget -q https://student.cs.uwaterloo.ca/~cs451/content/cs431/spam_eval.sh"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPJwvzrGqhii"
      },
      "source": [
        "Now compile this program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyXv9ef-qkoE"
      },
      "source": [
        "!gcc -w -O2 -o compute_spam_metrics compute_spam_metrics.c -lm"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59162Mlri7Oo"
      },
      "source": [
        " Given your ouput file, in the proper format, it will compute the area under the receiver operating curve (ROC).   This is a common way to characterize classifier error.    The lower this score, the better.   The evaluation program should produce one line of output, like this\n",
        "```\n",
        "1-ROCA%: 17.25\n",
        "```\n",
        "\n",
        "Use your classifier to classify the test instances using each of the three classification models that you produced, which should result in three different output files.   Then, in the cell below,\n",
        "use the evaluation program to evaluate your results.\n",
        "\n",
        "If you are working on your own machine, you can follow the instructions at the beginning of [CS451 A6](https://www.student.cs.uwaterloo.ca/~cs451/assignment6-451.html) to download and install the program. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKHo4O_yi7Oo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c670365f-44d5-44b6-a7b4-5803bb3724d0"
      },
      "source": [
        "# Your tests here\n",
        "#  Run the evaluation program like this, after first replacing \"output-file\"\n",
        "#  with the name of the folder that holds your classifier's output\n",
        "spark_classify(results_path='results/group_x_test_qrels')\n",
        "spark_classify(input_model='models/group_y_model', results_path='results/group_y_test_qrels')\n",
        "spark_classify(input_model='models/britney_model',  results_path='results/britney_test_qrels')\n",
        "!bash spam_eval.sh results/group_x_test_qrels/\n",
        "!bash spam_eval.sh results/group_y_test_qrels/\n",
        "!bash spam_eval.sh results/britney_test_qrels/"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1-ROCA%: 17.26\n",
            "1-ROCA%: 12.82\n",
            "1-ROCA%: 15.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hVKBrDDi7Oo"
      },
      "source": [
        "---\n",
        "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
      ]
    }
  ]
}