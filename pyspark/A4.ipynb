{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "## CS431/631 Data Intensive Distributed Computing\n",
    "### Fall 2020 - Assignment 4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "**Please edit this (text) cell to provide your name and UW student ID number!**\n",
    "* **Name:** Yu Li\n",
    "* **ID:** 20603929"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Overview\n",
    "For this assignment, you will be using Python and Spark to perform some simple analyses on relational (tabular) data.  You will use Spark to read tabular data from files and then answer simple queries about the data in those tables.\n",
    "\n",
    "In addition to Python and Spark, you will need to use a little bit of SQL.  If you are already familiar with SQL, great.  If not, you will want to spend a short amount of time getting familiar with the basics.   Type \"SQL tutorial\" into your favorite search engine, and you will find many examples of text, interactive and video tutorials.   As a simple starting point, you might also want to look at [these slides](https://cs.uwaterloo.ca/~kmsalem/courses/cs743/F14/slides/sql.pdf), which give a short introduction to SQL.   Even this is much more than you will need for this assignment.\n",
    "\n",
    "You will be working with tabular data based on the schema for the TPC-H benchmark, which is a standard test used to measure the performance of relational database systems.   The schema defines the tables that exist in the database, the information in each table, and relationships between information in one table and information in another.   The TPC-H schema models business information:  customers, orders, parts, suppliers, and so on. You can find it on page 13 of the [TPC-H benchmark specification](http://www.tpc.org/TPC_Documents_Current_Versions/pdf/tpc-h_v2.17.3.pdf).   The TPC-H schema is important for this assignment, so make sure that you keep this schema handy.\n",
    "\n",
    "Finally, for this assignment you will be using Spark in a slightly different way than you have used it so far.  For previous assignments, you have used the original Spark RDD interface.   For this assignment, you will be using the newer Spark interface, which is based on DataFrames.   DataFrames are RDDs in which each element is constrained to have the same structure.   You can think of a DataFrame like a table in a relational database.   Each element of the DataFrame is a row or record in the table.   All records have the same structure.   There is a programming guide for Spark DataFrames [here](https://spark.apache.org/docs/latest/sql-programming-guide.html).  Start with that. There is also a [more detailed guide to the full programming interface](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html).\n",
    "\n",
    "---\n",
    "#### Getting Started\n",
    "To get started, let's initialize Spark, load a couple of the TPC-H tables, and run some simple queries.\n",
    "First, as always, we need to tell the Python interpreter where to find Spark, so run `findspark.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Next, we launch Spark.  When you used the RDD interface for previous assignments, you created a `SparkContext` when you launched Spark.   To use Spark SQL and the DataFrame interface, you instead create a `SparkSession`.   You do that as shown in the next cell (run it!).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "spark = SparkSession.builder.appName(\"YourTest\").master(\"local[2]\").config('spark.ui.port', random.randrange(4000,5000)).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Next, let's create DataFrames from the TPC-H data files.  We have installed the TPC-H data files in the directory `/u/cs451/data/TPC-H-0.1-TXT/`. \n",
    "\n",
    "If you are working on your own machine, you can download the dataset [here](https://roegiest.com/bigdata-2019w/assignments/TPC-H-0.1-TXT.tar.gz) and extract it to the same location on your system, for convenience. If you decide to modify the location of the dataset, you will have to change the path provided to `spark.read.csv()` for the examples that follow. You will be able to provide a different path when answering the questions.\n",
    "\n",
    "There is one file for each table in the TPC-H database, e.g., `nation.tbl` for the TPC-H Nation table, `customer.tbl` for the TPC-H Customer table, and so on.    These are plain text csv files, with the character `|` used as a field separator.\n",
    "\n",
    "Create a Spark DataFrame corresponding to the TPC-H Nation table by loading the data from the `nation.tbl` file.   Run the code in the next cell to do this.   After you run this code, `nation_raw` will refer to your new Spark DataFrame.   The Spark `show()` method will display a small (default 20) number of elements from the DataFrame, so that you can inspect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+--------------------+----+\n",
      "|_c0|       _c1|_c2|                 _c3| _c4|\n",
      "+---+----------+---+--------------------+----+\n",
      "|  0|   ALGERIA|  0| haggle. carefull...|null|\n",
      "|  1| ARGENTINA|  1|al foxes promise ...|null|\n",
      "|  2|    BRAZIL|  1|y alongside of th...|null|\n",
      "|  3|    CANADA|  1|eas hang ironic, ...|null|\n",
      "|  4|     EGYPT|  4|y above the caref...|null|\n",
      "|  5|  ETHIOPIA|  0|ven packages wake...|null|\n",
      "|  6|    FRANCE|  3|refully final req...|null|\n",
      "|  7|   GERMANY|  3|l platelets. regu...|null|\n",
      "|  8|     INDIA|  2|ss excuses cajole...|null|\n",
      "|  9| INDONESIA|  2| slyly express as...|null|\n",
      "| 10|      IRAN|  4|efully alongside ...|null|\n",
      "| 11|      IRAQ|  4|nic deposits boos...|null|\n",
      "| 12|     JAPAN|  2|ously. final, exp...|null|\n",
      "| 13|    JORDAN|  4|ic deposits are b...|null|\n",
      "| 14|     KENYA|  0| pending excuses ...|null|\n",
      "| 15|   MOROCCO|  0|rns. blithely bol...|null|\n",
      "| 16|MOZAMBIQUE|  0|s. ironic, unusua...|null|\n",
      "| 17|      PERU|  1|platelets. blithe...|null|\n",
      "| 18|     CHINA|  2|c dependencies. f...|null|\n",
      "| 19|   ROMANIA|  3|ular asymptotes a...|null|\n",
      "+---+----------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/nation.tbl\",sep='|',inferSchema=True)\n",
    "nation_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Now you have a DataFrame to work with.   The columns of the DataFrame correspond to the fields of the TPC-H Nation table, so have a look at the TPC-H schema diagram to see what you are dealing with.   Column c0 is the NATIONKEY, column c1 is the NAME, c2 is the REGIONKEY, and so on.   Since this is a synthetic database, you'll notice that the data in some of the fields (like the COMMENT field) consists of random words.   That's fine.   You can also ask Spark to tell you about the type of data in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'int'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Before going on, let's clean this DataFrame up a bit, to make it easier to use.   First, let's assign names to the columns, so that we can remember what information each column holds.   Second, you'll notice that Spark has created an extra final column (filled with `null` values) because each line in the input file ends with a separator character (|).  Let's drop that final column, since we don't need it.   Run the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------------------+\n",
      "|NationKey|      Name|RegionKey|             Comment|\n",
      "+---------+----------+---------+--------------------+\n",
      "|        0|   ALGERIA|        0| haggle. carefull...|\n",
      "|        1| ARGENTINA|        1|al foxes promise ...|\n",
      "|        2|    BRAZIL|        1|y alongside of th...|\n",
      "|        3|    CANADA|        1|eas hang ironic, ...|\n",
      "|        4|     EGYPT|        4|y above the caref...|\n",
      "|        5|  ETHIOPIA|        0|ven packages wake...|\n",
      "|        6|    FRANCE|        3|refully final req...|\n",
      "|        7|   GERMANY|        3|l platelets. regu...|\n",
      "|        8|     INDIA|        2|ss excuses cajole...|\n",
      "|        9| INDONESIA|        2| slyly express as...|\n",
      "|       10|      IRAN|        4|efully alongside ...|\n",
      "|       11|      IRAQ|        4|nic deposits boos...|\n",
      "|       12|     JAPAN|        2|ously. final, exp...|\n",
      "|       13|    JORDAN|        4|ic deposits are b...|\n",
      "|       14|     KENYA|        0| pending excuses ...|\n",
      "|       15|   MOROCCO|        0|rns. blithely bol...|\n",
      "|       16|MOZAMBIQUE|        0|s. ironic, unusua...|\n",
      "|       17|      PERU|        1|platelets. blithe...|\n",
      "|       18|     CHINA|        2|c dependencies. f...|\n",
      "|       19|   ROMANIA|        3|ular asymptotes a...|\n",
      "+---------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation = nation_raw.toDF('NationKey','Name','RegionKey','Comment','extra').drop('extra').cache()\n",
    "nation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "This style of code should look familar to you.  We started with the `nation_raw` DataFrame and applied a series of DataFrame operations (`toDF`, `drop`, and `cache`).   This is just like the RDD interface, except now we are applying DataFrame operations to DataFrames, instead of RDD operations to RDDs.\n",
    "\n",
    "Next, let's load up the TPC-H Supplier table, and then try performing some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|SuppKey|              Name|             Address|NationKey|          Phone|AcctBal|             Comment|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|      1|Supplier#000000001| N kD4on9OM Ipw3,...|       17|27-918-335-1736|5755.94|each slyly above ...|\n",
      "|      2|Supplier#000000002|89eJ5ksX3ImxJQBvx...|        5|15-679-861-2259|4032.68| slyly bold instr...|\n",
      "|      3|Supplier#000000003|q1,G3Pj6OjIuUYfUo...|        1|11-383-516-1199| 4192.4|blithely silent r...|\n",
      "|      4|Supplier#000000004|Bk7ah4CK8SYQTepEm...|       15|25-843-787-7479|4641.08|riously even requ...|\n",
      "|      5|Supplier#000000005|   Gcdm2rJRzl5qlTVzc|       11|21-151-690-3663|-283.84|. slyly regular p...|\n",
      "|      6|Supplier#000000006|        tQxuVm7s7CnK|       14|24-696-997-4969|1365.79|final accounts. r...|\n",
      "|      7|Supplier#000000007|s,4TicNGB4uO6PaSq...|       23|33-990-965-2201|6820.35|s unwind silently...|\n",
      "|      8|Supplier#000000008|9Sq4bBH2FQEmaFOoc...|       17|27-498-742-3860|7627.85|al pinto beans. a...|\n",
      "|      9|Supplier#000000009|1KhUgZegwM3ua7dsY...|       10|20-403-398-8662|5302.37|s. unusual, even ...|\n",
      "|     10|Supplier#000000010|  Saygah3gYWMp72i PY|       24|34-852-489-8585|3891.91|ing waters. regul...|\n",
      "|     11|Supplier#000000011|    JfwTs,LZrV, M,9C|       18|28-613-996-1505|3393.08|y ironic packages...|\n",
      "|     12|Supplier#000000012|         aLIW  q0HYd|        8|18-179-925-7181|1432.69|al packages nag a...|\n",
      "|     13|Supplier#000000013|HK71HQyWoqRWOX8GI...|        3|13-727-620-7813|9107.22|requests engage r...|\n",
      "|     14|Supplier#000000014|     EXsnO5pTNj4iZRm|       15|25-656-247-5058|9189.82|l accounts boost....|\n",
      "|     15|Supplier#000000015|olXVbNBfVzRqgokr1...|        8|18-453-357-6394| 308.56| across the furio...|\n",
      "|     16|Supplier#000000016|YjP5C55zHDXL7LalK...|       22|32-822-502-4215|2972.26|ously express ide...|\n",
      "|     17|Supplier#000000017|c2d,ESHRSkK3WYnxp...|       19|29-601-884-9219|1687.81|eep against the f...|\n",
      "|     18|Supplier#000000018|    PGGVE5PWAMwKDZw |       16|26-729-551-1115|7040.82|accounts snooze s...|\n",
      "|     19|Supplier#000000019|edZT3es,nBFD8lBXT...|       24|34-278-310-2731|6150.38|refully final fox...|\n",
      "|     20|Supplier#000000020|iybAE,RmTymrZVYaF...|        3|13-715-945-6730| 530.82|n, ironic ideas w...|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supplier_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
    "supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
    "supplier.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Writing Queries\n",
    "There are two equivalent ways of writing queries over Spark DataFrames.   The first way is to assign a \"view name\" to the DataFrame, and then write SQL queries referring to those view names using the `sql` operation.  \n",
    "\n",
    "The code below gives the view names \"nation\" and \"supplier\" to the two DataFrames we've already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "supplier.createOrReplaceTempView(\"supplier\")\n",
    "nation.createOrReplaceTempView(\"nation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Now, we can write SQL queries that refer to the \"supplier\" and \"nation\" views as tables.   For example, suppose we want to see the names and addresses of suppliers who have account balances above 9900.00:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+\n",
      "|              Name|             Address|AcctBal|\n",
      "+------------------+--------------------+-------+\n",
      "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
      "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
      "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
      "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
      "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_result = spark.sql(\"select Name, Address, AcctBal from supplier where AcctBal > 9900.00\")\n",
    "q1_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "In the example above, the `sql` command runs the SQL query against the supplier table.   It returns the query result as a new DataFrame, which `q1_result` refers to.\n",
    "\n",
    "Instead of writing your queries in SQL and running them using `sql`, it is possible to do the same thing by applying a sequence of DataFrame operations to the input DataFrames, as you did when you were using the RDD interface in the previous assignments.    For example, to answer the same query that we just answered using SQL, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+\n",
      "|              Name|             Address|AcctBal|\n",
      "+------------------+--------------------+-------+\n",
      "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
      "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
      "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
      "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
      "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_resultB = supplier.filter(\"AcctBal > 9900.00\").select('Name','Address','AcctBal')\n",
    "q1_resultB.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "Both methods should give the same result.   Internally, Spark handles both similarly.   For this assignment, you'll be asked to try out both methods.\n",
    "\n",
    "Now it is time for you to try writing your own queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "### Important\n",
    "\n",
    "###### The questions that follow ask you to implement functions whose prototypes are given to you. Do **not** change the prototypes of the functions. Do **not** write code outside of the functions. \n",
    "\n",
    "##### You may use specific cells, identified by `# Your tests here`, for test purposes. Code in these cells will *not* be executed when marking your assignment. \n",
    "\n",
    "##### We will make sure the function `load_dataset_and_set_views()` is called before your answers to other questions.\n",
    "\n",
    "---\n",
    "#### Question 0\n",
    "First, you will instantiate the DataFrames and set the views needed to answer the following questions. \n",
    "Modify the code in the cell below to read the corresponding tables from disk in the same fashion as the supplier table. Keep the column names consistent with the [TPC-H schema](http://www.tpc.org/TPC_Documents_Current_Versions/pdf/tpc-h_v2.17.3.pdf) (page 13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "tags": [
     "setup",
     "global",
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def load_dataset_and_set_views(path=\"/u/cs451/data/TPC-H-0.1-TXT/\"):\n",
    "    global supplier, orders, customer, partsupp, nation, part\n",
    "    \n",
    "    supplier_raw = spark.read.csv(path+\"supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
    "    supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
    "    supplier.createOrReplaceTempView(\"supplier\")\n",
    "    \n",
    "    # Your solution to Question 0 here\n",
    "    order_raw = spark.read.csv(path+\"orders.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
    "    orders = order_raw.toDF(\"OrderKey\",\"CustKey\",\"OrderStatus\",\"TotalPrice\",\"OrderDate\",\"OrderPriority\",\"Clerk\", \"ShipPriority\", \"Comment\").cache()\n",
    "    orders.createOrReplaceTempView(\"orders\")\n",
    "    \n",
    "    customer_raw = spark.read.csv(path+\"customer.tbl\",sep='|',inferSchema=True).drop(\"_c8\")\n",
    "    customer = customer_raw.toDF(\"CustKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"MktSegment\",\"Comment\").cache()\n",
    "    customer.createOrReplaceTempView(\"customer\")\n",
    "    \n",
    "    partsupp_raw = spark.read.csv(path+\"partsupp.tbl\",sep='|',inferSchema=True).drop(\"_c5\")\n",
    "    partsupp = partsupp_raw.toDF(\"PartKey\",\"SuppKey\",\"AvailQty\",\"SupplyCost\",\"Comment\").cache()\n",
    "    partsupp.createOrReplaceTempView(\"partsupp\")\n",
    "    \n",
    "    nation_raw = spark.read.csv(path+\"nation.tbl\",sep='|',inferSchema=True).drop(\"_c4\")\n",
    "    nation = nation_raw.toDF(\"NationKey\",\"Name\",\"RegionKey\", \"Comment\").cache()\n",
    "    nation.createOrReplaceTempView(\"nation\")\n",
    "    \n",
    "    part_raw = spark.read.csv(path+\"part.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
    "    part = part_raw.toDF(\"PartKey\",\"Name\",\"MFGR\",\"Brand\",\"Type\", \"Size\", \"Container\", \"RetailPrice\", \"Comment\").cache()\n",
    "    part.createOrReplaceTempView(\"part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "tags": [
     "test",
     "setup"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----------+----------+-------------------+-------------+---------------+------------+--------------------+\n",
      "|OrderKey|CustKey|OrderStatus|TotalPrice|          OrderDate|OrderPriority|          Clerk|ShipPriority|             Comment|\n",
      "+--------+-------+-----------+----------+-------------------+-------------+---------------+------------+--------------------+\n",
      "|       1|   3691|          O| 194029.55|1996-01-02 00:00:00|        5-LOW|Clerk#000000951|           0|nstructions sleep...|\n",
      "|       2|   7801|          O|  60951.63|1996-12-01 00:00:00|     1-URGENT|Clerk#000000880|           0| foxes. pending a...|\n",
      "|       3|  12332|          F| 247296.05|1993-10-14 00:00:00|        5-LOW|Clerk#000000955|           0|sly final account...|\n",
      "|       4|  13678|          O|  53829.87|1995-10-11 00:00:00|        5-LOW|Clerk#000000124|           0|sits. slyly regul...|\n",
      "|       5|   4450|          F| 139660.54|1994-07-30 00:00:00|        5-LOW|Clerk#000000925|           0|quickly. bold dep...|\n",
      "+--------+-------+-----------+----------+-------------------+-------------+---------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "load_dataset_and_set_views()\n",
    "q1 = spark.sql(\"select * from orders limit 5\")\n",
    "q1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 1 (2/25 marks)\n",
    "In the cell below, implement the function `five_highest_totalprice_orders_sql()` that will return the ORDERKEY, ORDERDATE, and TOTALPRICE of the five orders with the highest TOTALPRICE. Express your query in SQL, and use `sql` to execute it. The function must return a DataFrame with the corresponding schema `ORDERKEY, ORDERDATE, TOTALPRICE`. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def five_highest_totalprice_orders_sql():\n",
    "    query = \"select OrderKey, OrderDate, TotalPrice from orders order by TotalPrice DESC limit 5\"\n",
    "    return spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+\n",
      "|OrderKey|          OrderDate|TotalPrice|\n",
      "+--------+-------------------+----------+\n",
      "|  279812|1994-02-19 00:00:00| 479129.21|\n",
      "|  370726|1996-09-29 00:00:00|  460099.4|\n",
      "|   66659|1993-10-15 00:00:00| 458396.42|\n",
      "|  253639|1998-01-23 00:00:00| 456532.89|\n",
      "|  502886|1994-04-12 00:00:00| 456423.88|\n",
      "+--------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "five_highest_totalprice_orders_sql().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 2 (2/25 marks)\n",
    "In the cell below, implement the function `five_highest_totalprice_orders_dtf()` to answer the same query you answered in Question 1, but this time do not use the `sql` method. The function must returns a DataFrame with the corresponding schema `ORDERKEY, ORDERDATE, TOTALPRICE`, ordered by TOTALPRICE value. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def five_highest_totalprice_orders_dtf():\n",
    "    return orders.select('OrderKey', \"OrderDate\", \"TotalPrice\").orderBy(\"TotalPrice\", ascending=False).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+\n",
      "|OrderKey|          OrderDate|TotalPrice|\n",
      "+--------+-------------------+----------+\n",
      "|  279812|1994-02-19 00:00:00| 479129.21|\n",
      "|  370726|1996-09-29 00:00:00|  460099.4|\n",
      "|   66659|1993-10-15 00:00:00| 458396.42|\n",
      "|  253639|1998-01-23 00:00:00| 456532.89|\n",
      "|  502886|1994-04-12 00:00:00| 456423.88|\n",
      "+--------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "five_highest_totalprice_orders_dtf().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 3 (3/25 marks)\n",
    "In the cell below, implement the function `cust_most_recent_order_sql(custkey)` that takes a Customer Key as an input and returns the customer's name as well as the ORDERDATE and TOTALPRICE of that customer's most recent order.   Express the query in SQL, and use `sql` to execute it. You will need to use information from multiple tables to generate your answer. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def cust_most_recent_order_sql(custkey):\n",
    "    query = \"\"\"select c.Name, \n",
    "                      o.OrderDate, \n",
    "                      o.TotalPrice \n",
    "                      from customer as c \n",
    "                      inner join \n",
    "                      orders as o \n",
    "                      on c.CustKey = o.CustKey where c.CustKey = {0}\n",
    "                      order by OrderDate Desc\n",
    "                      limit 1\n",
    "                      \"\"\".format(custkey)\n",
    "    return spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+----------+\n",
      "|              Name|          OrderDate|TotalPrice|\n",
      "+------------------+-------------------+----------+\n",
      "|Customer#000000001|1997-03-04 00:00:00| 268835.44|\n",
      "+------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "cust_most_recent_order_sql(1).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 4 (3/25 marks)\n",
    "In the cell below, answer the same query you answered in Question 3, but this time do not use the `sql` method. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def cust_most_recent_order_dtf(custkey):\n",
    "    order_q = orders.filter(\"CustKey=\"+str(custkey)).select(\"CustKey\",\"OrderDate\", \"TotalPrice\").orderBy(\"OrderDate\", ascending=False).limit(1)\n",
    "    cust_q = customer.filter(\"CustKey=\"+str(custkey)).select(\"CustKey\", \"Name\")\n",
    "    return cust_q.join(order_q, on = \"CustKey\").select(\"Name\", \"OrderDate\", \"TotalPrice\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+----------+\n",
      "|              Name|          OrderDate|TotalPrice|\n",
      "+------------------+-------------------+----------+\n",
      "|Customer#000000001|1997-03-04 00:00:00| 268835.44|\n",
      "+------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "cust_most_recent_order_dtf(1).show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 5 (5/25 marks)\n",
    "In the cell below, implement `distinct_supplied_parts(nname)` that will return the number of distinct parts supplied by suppliers that are located in the given nation. The function must output an **integer** (not a DataFrame).\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def distinct_supplied_parts(nname):\n",
    "    q_1 = nation.filter(\"Name = '{0}'\".format(nname)).select(\"nationkey\")\\\n",
    "                                        .join(supplier, on = \"nationkey\")\\\n",
    "                                        .join(partsupp, on = \"suppkey\")\\\n",
    "                                        .select(\"partkey\")\\\n",
    "                                        .distinct().count()\n",
    "    return q_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2799"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your tests here\n",
    "distinct_supplied_parts(\"CANADA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 6 (5/25 marks)\n",
    "In the cell below, implement `count_suppliers_brand_per_nation(bname)` that takes a brand name as input, like those that appear in the Parts table.  Given the brand,\n",
    "report, for each nation, a count of that nation's suppliers of parts having that brand, ordered by nation name.   Your output should be a table (i.e. DataFrame) of nations and their supplier counts. Each supplier should be counted at most once in a nation's total, even if that supplier produces multiple parts with the given brand.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def count_suppliers_brand_per_nation(bname):\n",
    "    # Your solution to Question 6 here\n",
    "    q = \"\"\" select  n.name, count(distinct ps.suppkey) as count from nation as n inner join\n",
    "            supplier as s on n.nationkey = s.nationkey\n",
    "            inner join partsupp as ps on s.suppkey = ps.suppkey\n",
    "            inner join part as p on ps.partkey = p.partkey\n",
    "            where p.brand = '{0}' group by n.name order by n.name\"\"\".format(bname)\n",
    "    return spark.sql(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      name|count|\n",
      "+----------+-----+\n",
      "|   ALGERIA|   34|\n",
      "| ARGENTINA|   38|\n",
      "|    BRAZIL|   41|\n",
      "|    CANADA|   35|\n",
      "|     CHINA|   51|\n",
      "|     EGYPT|   39|\n",
      "|  ETHIOPIA|   32|\n",
      "|    FRANCE|   35|\n",
      "|   GERMANY|   49|\n",
      "|     INDIA|   45|\n",
      "| INDONESIA|   45|\n",
      "|      IRAN|   39|\n",
      "|      IRAQ|   40|\n",
      "|     JAPAN|   40|\n",
      "|    JORDAN|   28|\n",
      "|     KENYA|   35|\n",
      "|   MOROCCO|   40|\n",
      "|MOZAMBIQUE|   32|\n",
      "|      PERU|   37|\n",
      "|   ROMANIA|   32|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "count_suppliers_brand_per_nation(\"Brand#14\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "#### Question 7 (5/25 marks)\n",
    "In the cell below, write code for the function `order_number_per_customer_nation(nname)` that will return, for a given nation name, for each year, the total number of orders placed by customers from the specified Nation, ordered by descending number of orders.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer. Use the view(s) or DataFrame(s) created in Question 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "tags": [
     "code"
    ]
   },
   "outputs": [],
   "source": [
    "def order_number_per_customer_nation(nname):\n",
    "    q = \"\"\"select  year(o.orderdate) as year , count(distinct o.orderkey) as count from orders as o\n",
    "           inner join customer as c on o.custkey = c.custkey inner join \n",
    "           nation as n on n.nationkey = c.nationkey where\n",
    "           n.Name = '{0}' group by year order by count desc\"\"\".format(nname)\n",
    "    return spark.sql(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|1992|  982|\n",
      "|1996|  940|\n",
      "|1995|  932|\n",
      "|1997|  921|\n",
      "|1994|  912|\n",
      "|1993|  900|\n",
      "|1998|  595|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your tests here\n",
    "order_number_per_customer_nation(\"CANADA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "instructions"
    ]
   },
   "source": [
    "---\n",
    "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment.\n",
    "\n",
    "**Make sure that your notebook runs properly on the Jupyter hub before submitting it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
